{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Tutorial\n",
    "\n",
    "IFT6135 â€“ Representation Learning\n",
    "\n",
    "A Deep Learning Course, January 2019\n",
    "\n",
    "By Chin-Wei Huang \n",
    "\n",
    "(Adapted from Sandeep Subramanian's 2017 MILA tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Your Own Modules\n",
    "\n",
    "### `torch.nn.module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``nn.Module`` is base class for all neural network modules.\n",
    "\n",
    "You should also write your modules as sub-class of ``nn.Module``, so that it can inherit the following attributes:\n",
    "\n",
    "* *Recursive structure*: you can wrap an instantiation of a Module class with another one, which stores the inner one as its parent\n",
    "\n",
    "* *Cudafiability*: you can easily cudafy the whole sequence of modules using `model.cuda()`\n",
    "\n",
    "* *Serializable*: you can save your trained model (checkpoint, early stopping ...) using ``torch.save``, ``torch.load``\n",
    "\n",
    "* *Parameters*: you can call model.parameters() to access all parameters at the same time. \n",
    "\n",
    "etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://pytorch.org/docs/master/nn.html#linear-layers\n",
    "class Linear(nn.Module):\n",
    "    r\"\"\"Applies a linear transformation to the incoming data: :math:`y = Ax + b`\n",
    "\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        bias: If set to False, the layer will not learn an additive bias. Default: True\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, in\\_features)`\n",
    "        - Output: :math:`(N, out\\_features)`\n",
    "\n",
    "    Attributes:\n",
    "        weight: the learnable weights of the module of shape (out_features x in_features)\n",
    "        bias:   the learnable bias of the module of shape (out_features)\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> m = nn.Linear(20, 30)\n",
    "        >>> input = autograd.Variable(torch.randn(128, 20))\n",
    "        >>> output = m(input)\n",
    "        >>> print(output.size())\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.bias is None:\n",
    "            return self._backend.Linear()(input, self.weight)\n",
    "        else:\n",
    "            return self._backend.Linear()(input, self.weight, self.bias)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "            + str(self.in_features) + ' -> ' \\\n",
    "            + str(self.out_features) + ')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(MyLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.Tensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.bias is None:\n",
    "            return torch.mm(input, self.weight) \n",
    "        else:\n",
    "            return torch.mm(input, self.weight) + self.bias\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  1,  1,  1],\n",
      "        [ 1,  1,  1,  1]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "x = torch.from_numpy(np.random.randn(2, 3)).float()\n",
    "\n",
    "\n",
    "linear1 = nn.Linear(3,4)\n",
    "linear2 = MyLinear(3,4)\n",
    "\n",
    "# set the weight and bias of linear2 to be the same as linear1's\n",
    "linear2.weight.data = linear1.weight.data.transpose(1,0)\n",
    "linear2.bias.data = linear1.bias.data\n",
    "\n",
    "print(torch.eq(linear1(x), linear2(x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet example\n",
    "\n",
    "* Resnet blocks let the gradient flow through the hidden unit more directly and at the same time increase expressiveness\n",
    "\n",
    "Res(x) = F(x, {W}) + x\n",
    "\n",
    "Res(x) = F(x, {W1}) + W2 x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, activation=nn.ReLU()):\n",
    "        super(ResLinear, self).__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        if in_features != out_features:\n",
    "            self.project_linear = nn.Linear(in_features, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        inner = self.activation(self.linear(x))\n",
    "        if self.in_features != self.out_features:\n",
    "            skip = self.project_linear(x)\n",
    "        else:    \n",
    "            skip = x\n",
    "        return inner + skip\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "x = torch.from_numpy(np.random.randn(2, 3)).float()\n",
    "\n",
    "\n",
    "res1 = nn.Linear(3,3)\n",
    "res2 = ResLinear(3,5)\n",
    "\n",
    "print(res1(x).size())\n",
    "print(res2(x).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting things altogether, Sequential, Parameter updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, Linear=ResLinear):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        self.predict_ = nn.Sequential(\n",
    "            Linear(784, 328),\n",
    "            nn.ReLU(),\n",
    "            Linear(328, 328),\n",
    "            nn.ReLU(),\n",
    "            Linear(328, 10),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def predict_proba(self, x):\n",
    "        return F.softmax(x)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return torch.max(self.predict_proba(x))[1]\n",
    "        # ``max'' returns (max_value, argmax)\n",
    "    \n",
    "    def loss(self, x, target):\n",
    "        proba = self.predict_(x)\n",
    "        return self.criterion(proba, target)\n",
    "\n",
    "# CrossEntropyLoss -> output no nonlinearity\n",
    "# NLLloss -> logsoftmax\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "caveate:    ``CrossEntropyLoss``    versus    ``NLLLoss``\n",
    "\n",
    "\n",
    "* ``CrossEntropyLoss`` takes in *pre-softmax* as input\n",
    "\n",
    "* ``NLLLoss`` takes in *log-softmax* as input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6893)\n",
      "tensor(2.6893)\n"
     ]
    }
   ],
   "source": [
    "y = torch.Tensor(1,10).normal_()\n",
    "t = torch.from_numpy(np.random.choice(10, size=1))\n",
    "\n",
    "loss1 = nn.CrossEntropyLoss()\n",
    "loss2 = nn.NLLLoss()\n",
    "\n",
    "print(loss1(y, t))\n",
    "print(loss2(nn.LogSoftmax(dim=1)(y), t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3574)\n"
     ]
    }
   ],
   "source": [
    "x = torch.from_numpy(np.random.randn(64, 784)).float()\n",
    "t = torch.from_numpy(np.random.choice(10, size=64))\n",
    "\n",
    "model = MyModel()\n",
    "print(model.loss(x, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Parameters (Manually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n",
      "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n",
      "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n",
      "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n",
      "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n",
      "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n",
      "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n",
      "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n",
      "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n",
      "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.from_numpy(np.random.randn(64, 784)).float()\n",
    "t = torch.from_numpy(np.random.choice(10, size=64))\n",
    "model = MyModel()\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "for i in range(10):\n",
    "    loss = model.loss(x, t)\n",
    "    loss.backward()\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        #param.data = param.data - lr*param.grad.data\n",
    "        param.data.sub_(param.grad.data*lr)\n",
    "        param.grad.data.zero_()\n",
    "        \n",
    "    #print(loss)\n",
    "    \n",
    "    print(param.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Parameters (``torch.optim``)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4130)\n",
      "tensor(1.6219)\n",
      "tensor(0.6712)\n",
      "tensor(0.1796)\n",
      "tensor(1.00000e-02 *\n",
      "       4.2157)\n",
      "tensor(1.00000e-02 *\n",
      "       1.0053)\n",
      "tensor(1.00000e-03 *\n",
      "       2.7863)\n",
      "tensor(1.00000e-04 *\n",
      "       9.3589)\n",
      "tensor(1.00000e-04 *\n",
      "       3.7458)\n",
      "tensor(1.00000e-04 *\n",
      "       1.7405)\n"
     ]
    }
   ],
   "source": [
    "x = torch.from_numpy(np.random.randn(64, 784)).float()\n",
    "t = torch.from_numpy(np.random.choice(10, size=64))\n",
    "model = MyModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = model.loss(x, t)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "        \n",
    "    print(loss)\n",
    "\n",
    "# what happens if you use momentum? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
